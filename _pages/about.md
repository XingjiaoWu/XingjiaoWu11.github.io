---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am Xing Jiao, a Ph.D. holder currently serving as a Super Postdoctoral Researcher in Shanghai. My research journey has primarily centered around the realms of computer vision, generative artificial intelligence, digital medicine, and human-in-the-loop computing.

With gratitude, I have had the privilege of contributing to the academic landscape through the publication of over 40 research papers in various international conferences and journals. Among them, I am humbled to share that 7 papers have found their place in SCI Q1/CCF A-tier venues, with 6 as the first author or corresponding author. An additional 12 papers have been accepted in SCI Q2/CCF B-tier outlets, with 7 as the first author or corresponding author.

I am genuinely appreciative of the recognition received for one of my papers in FGCS, honored with the Editorâ€™s Choice Papers awardâ€”an acknowledgment that reflects the collaborative effort within the academic community. Similarly, receiving the Best Paper Award at the McGE '23 conference, a CCF A-class workshop, is an acknowledgment that underscores the collective dedication to advancing knowledge.

In terms of project leadership, I have had the privilege of contributing to open projects at the Shanghai Multi-Dimensional Information Processing Key Laboratory and guiding the Academic Innovation Capability Enhancement Program for Outstanding Ph.D. Students at East China Normal University. Additionally, serving as the project lead for collaborative initiatives with Huawei Noah's Ark Laboratory has been a collaborative effort that I cherish.

I am sincerely grateful for the recognition received, including the First Prize in the 2022 Shanghai Technical Invention Award (ranking 13/15, the sole student recipient), and the Second Prize in the 2022 Annual Scientific and Technological Progress Award (ranking 6/10). Being recognized as a Rising Star of Chinese University Students during my doctoral studies and receiving the title of an Outstanding Graduate by the Shanghai Municipality are acknowledgments that I deeply value and appreciate.

In all humility, I am eager to continue contributing to the academic community, embracing opportunities for collaborative research, and furthering the knowledge base in these evolving areas of study. 

My research interest includes speech synthesis, neural machine translation and automatic music generation. I have published 50+ papers <a href='https://scholar.google.com/citations?user=BhA6vd0AAAAJ'><img src="https://img.shields.io/endpoint?logo=Google%20Scholar&url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2FXingjiaoWu%2FXingjiaoWu.github.io@google-scholar-stats%2Fgs_data_shieldsio.json&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a> 


# ğŸ”¥ News
- *2022.02*: &nbsp;ğŸ‰ğŸ‰ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2022.02*: &nbsp;ğŸ‰ğŸ‰ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 


# ğŸ“ Publications 
- 1.X. Wu, L. Xiao, Y. Sun, J. Zhang, T. Ma, L. He, A Survey of Human-in-the-loop for Machine Learning, Future Generation Computer Systems,2022.[ Editorâ€™s Choice Papers (ç¼–å§”ä¼šä»è¿‘äº”å¹´å‘è¡¨è®ºæ–‡ä¸­æåï¼Œåè¯„é€‰å‡º20ç¯‡ï¼Œå…¥é€‰ç‡çº¦0.5% ) ]ï¼ˆä¸­ç§‘é™¢ä¸€åŒºï¼ŒIF=7.5ï¼‰
- 2.Y. Wan, W. Li, X. Wu*, J. Xu, and J. Yang. Auto-matic Image Aesthetic Assessment for Human-designed Digital Images. McGE â€™23.2023.[Best paper] (é€šè®¯ä½œè€…ï¼ŒCCF Aç±»ä¼šè®®Workshop)
- 3.X. Wu, Y. Zheng, T. Ma, H. Ye, L. He, Document Image Layout Analysis via Explicit Edge Embedding Network, Information Sciences, 2021.ï¼ˆä¸­ç§‘é™¢ä¸€åŒºï¼ŒIF=8.1ï¼‰
- 4.X. Wu, T. Ma, X. Du, Z. Hu, J. Yang, L. He, DRFN: A unified framework for complex document layout analysis, Information Processing & Management., 2023.ï¼ˆä¸­ç§‘é™¢ä¸€åŒºï¼ŒIF=8.6ï¼‰
- 5.X. Wu, L. Xiao, X. Du, Y. Zheng, X. Li, T. Ma, C. Jin, L. He,Cross-domain document layout analysis using document style guide, Expert Systems with Applicationsï¼Œ2024ï¼ˆä¸­ç§‘é™¢ä¸€åŒºï¼ŒIF=8.5ï¼‰
- 6.L. Xiao, X. Wu*, J. Xu, W. Li, C. Jin, L. He. Atlantis: Aesthetic-oriented Multiple Granularities Fusion Network for Joint Multimodal Aspect-based Sentiment Analysis, Information Fusion, 2024.ï¼ˆé€šè®¯ä½œè€…ï¼Œä¸­ç§‘é™¢ä¸€åŒºï¼ŒIF=18.6ï¼‰
- 7.L. Xiao, X. Wu*, S. Yang, J. Xu, J. Zhou, L. He. Cross-modal Fine-grained Alignment and Fusion Network for Multimodal Aspect-based Sentiment Analysis, Information Processing & Management., 2023.ï¼ˆé€šè®¯ä½œè€…ï¼Œä¸­ç§‘é™¢ä¸€åŒºï¼ŒIF=8.6ï¼‰
- 8.X. Wu, Z. Hu, X. Du, J. Yang, L. He. Document Layout Analysis via Dynamic Residual Feature Fusion. ICME, 2021.ï¼ˆCCF Bç±»ä¼šè®® Oralï¼‰
- 9.T. Ma, X. Wuâ€ , X.Du, Y. Wang, C. Jin, Image Layer Modeling for Complex Document Layout Generation, ICME, 2023.ï¼ˆå…±ä¸€ï¼ŒCCF Bç±»ä¼šè®®ï¼‰
- 10.J. Zhang, Z. Zhuang, L. Xiao, X. Wu*, T. Ma, L. He, Dual-Expert Distillation Network for Few-Shot Segmentation. ICME, 2023.ï¼ˆé€šè®¯ä½œè€…ï¼ŒCCF Bç±»ä¼šè®®ï¼‰
- 11.X. Wu, Y. Zheng, H. Ye, W. Hu, J. Yang, L. He, Adaptive Scenario Discovery for Crowd Counting. ICASSP, 2019.ï¼ˆCCF Bç±»ä¼šè®®ï¼‰
- 12.X. Wu, B. Xu, Y. Zheng, H. Ye, J. Yang, L. He . Fast video crowd counting with a Temporal Aware Network. Neurocomputing, 2020.ï¼ˆä¸­ç§‘é™¢äºŒåŒºï¼ŒIF=6.0ï¼‰
- 13.X. Wu, Y. Zheng, H. Ye, W. Hu, T. Ma, J. Yang, L. He, Counting Crowds with Varying Densities via Adaptive Scenario Discovery Framework[J], Neurocomputing , 2020.ï¼ˆä¸­ç§‘é™¢äºŒåŒºï¼ŒIF=6.0ï¼‰
- 14.L. Xiao, X. Wuâ€ , W. Wu ,J. Yang, L. He. Multi-channel Attentive Graph Convolutional Network With Sentiment Fusion For Multimodal Sentiment Analysis. ICASSP, 2022.ï¼ˆå…±ä¸€ï¼ŒCCF Bç±»ä¼šè®®ï¼‰
- 15.J. He, X. Wuâ€ , W. Hu, J. Yang, LSTMVA: vivid layout via LSTM-based Variational Autoencoder framework. ICDAR, 2021.ï¼ˆå…±ä¸€ï¼ŒCCF Cç±»ä¼šè®® æ–‡æ¡£å¤„ç†é¡¶çº§ä¼šè®®ï¼‰
- 16.X. Wu, S. Kong, Y. Zheng, H. Ye, J. Yang, L. He, Feature channel enhancement for crowd counting. IET Image Processing, 2020.ï¼ˆä¸­ç§‘é™¢å››åŒºï¼ŒIF=2.3ï¼‰
- 17.å´ä¿Šæ–Œ,å´æ™Ÿ,å´å…´è›Ÿ*.ä¸€ç§ç”¨äºæ±‚è§£TSPé—®é¢˜çš„éšæœºæœ€ä½³æ’å…¥çƒŸèŠ±ç®—æ³•[J].è®¡ç®—æœºå·¥ç¨‹ä¸ç§‘å­¦,2020,42(11):2080-2087.ï¼ˆCCF C (ä¸­æ–‡)ï¼Œä¸­æ–‡æ ¸å¿ƒï¼Œé€šè®¯ä½œè€…ï¼‰

# ğŸ– Honors and Awards
- 2022å¹´åº¦ä¸Šæµ·å¸‚ç§‘å­¦æŠ€æœ¯å¥–ï¼ˆæŠ€æœ¯å‘æ˜å¥–ä¸€ç­‰å¥–ï¼‰ï¼ˆä¸Šæµ·å¸‚äººæ°‘æ”¿åºœï¼‰ ï¼ˆ2023ï¼Œæ’å13/15ï¼Œå”¯ä¸€å­¦ç”Ÿï¼‰
- 2022å¹´åº¦ä¸Šæµ·å¸‚ç§‘å­¦æŠ€æœ¯å¥–ï¼ˆç§‘æŠ€è¿›æ­¥å¥–äºŒç­‰å¥–ï¼‰ï¼ˆä¸Šæµ·å¸‚äººæ°‘æ”¿åºœï¼‰ ï¼ˆ2023ï¼Œæ’å6/10ï¼‰
- 2020ä¸­å›½å¤§å­¦ç”Ÿè‡ªå¼ºä¹‹æ˜Ÿï¼ˆå…±é’å›¢ä¸­å¤®ã€å…¨å›½å­¦è”ï¼‰ (2021ï¼Œåå¸ˆå¤§å½“å¹´åº¦å”¯ä¸€å…¥é€‰)
- ä¸Šæµ·å¸‚ä¼˜ç§€æ¯•ä¸šç”Ÿï¼ˆä¸Šæµ·å¸‚æ•™è‚²å§”å‘˜ä¼šï¼‰ï¼ˆ2022ï¼‰
- ä¸­å›½ç ”ç©¶ç”Ÿæ•°å­¦å»ºæ¨¡ç«èµ›è‹±æ‰ï¼ˆä¸­å›½ç ”ç©¶ç”Ÿæ•°å­¦å»ºæ¨¡ç«èµ›ç»„å§”ä¼šï¼‰ï¼ˆ2023ï¼Œ20å¹´è¯„é€‰å‡º30äººï¼‰
- 2023å¹´CCFæŠ€æœ¯å…¬ç›Šé»‘å®¢é©¬æ‹‰æ¾å¤§èµ›ä¸­è£è·æœ€ä½³æ–¹æ¡ˆå¥–ï¼ˆä¸­å›½è®¡ç®—æœºå­¦ä¼šï¼‰ ï¼ˆ2023ï¼Œå…¨åœºå¾—åˆ†æœ€é«˜ï¼‰
- 2021-2022å¹´åº¦åä¸œå¸ˆèŒƒå¤§å­¦ä¼˜ç§€å­¦ä½è®ºæ–‡ï¼ˆåä¸œå¸ˆèŒƒå¤§å­¦ï¼‰ ï¼ˆ2022ï¼‰
- åä¸œå¸ˆèŒƒå¤§å­¦ä¼˜ç§€å­¦ç”Ÿå¹²éƒ¨ï¼ˆåä¸œå¸ˆèŒƒå¤§å­¦ï¼‰ (2021)ï¼Œåä¸œå¸ˆèŒƒå¤§å­¦ä¼˜ç§€å­¦ç”Ÿï¼ˆåä¸œå¸ˆèŒƒå¤§å­¦ï¼‰ (2020)
- å…¨å›½å¤§å­¦ç”Ÿç”µå·¥æ•°å­¦å»ºæ¨¡ç«èµ›ä¸€ç­‰å¥–ï¼ˆä¸­å›½ç”µæœºå·¥ç¨‹å­¦ä¼šç”µå·¥æ•°å­¦ä¸“å§”ä¼šï¼‰ (2017)

# ğŸ“– Educations
*2018.9-2022.6	åä¸œå¸ˆèŒƒå¤§å­¦ 	è®¡ç®—æœºåº”ç”¨æŠ€æœ¯     å·¥å­¦åšå£«

ç ”ç©¶æ–¹å‘ï¼šå¤šåª’ä½“ä¿¡æ¯å¤„ç†ï¼Œè®¡ç®—æœºè§†è§‰ï¼Œäººæœºæ··åˆ

*2015.9-2018.6 	æ˜†æ˜ç†å·¥å¤§å­¦ 	è®¡ç®—æœºè½¯ä»¶ä¸ç†è®º   å·¥å­¦ç¡•å£«

ç ”ç©¶æ–¹å‘ï¼šç®—æ³•ç ”ç©¶ï¼Œè½¯ä»¶å¼€å‘

*2011.9-2015.6 	æ˜†æ˜ç†å·¥å¤§å­¦ 	è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯   å·¥å­¦å­¦å£«
# ğŸ’¬ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/)

# ğŸ’» Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China.
